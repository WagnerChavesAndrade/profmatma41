---
title: "Estimadores Pontuais - Relatório 06"
author: "Wagner Chaves Andrade"
date: "07/11/2024"
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}
    \includegraphics[width=2in,height=2in]{ufsj.png}\LARGE\\}
  - \posttitle{\end{center}}
toc-title: "Sumário"
output:
  
  html_document:
    theme: journal
    highlight: tango
    toc: yes
    number_sections: yes
    includes:
      in_header: logo.html
  pdf_document:
    
    toc: yes
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
--- 

# Introdução
Uma das realidades da amostragem aleatória é que, quando se extraem repetidas amostras da mesma população, há tendência da estatística amostral variar de uma amostra para outra, e também em realação ao verdadeiro valor do parâmetro, simplesmente em razão de fatores casuais relacionados com amostragem.

Essa tendência é conhecida como variabilidade amostal. Assim quando fazemos inferências sobre uma população devemos considerar a variabilidade amostral.

Sempre que faz uma estimação de uma variável aleatória, existe uma probabilidade de se errar essa estimação quando comparamos o valor estimado com o valor que realmente foi observado.

O uso da amostra nos permite inferir os parâmetros da população a partir dos parâmetros da amostra através das chamadas Estimação Pontual e Estimação Intervalar.

# Estimação Pontual.
A necessidade de encontrarmos estimadores para determinados parâmetros se deve a diversas aplicações. Por exemplo, os estimadores apresentados até agora, nos informam um ponto amostral (estimador) que representará um ponto populacional (parâmetro). A este, denominamos de estimador
pontual.

**Definição:**

Qualquer estatística cujos valores são usados para estimar τ(θ), em que τ(.) é alguma função
do parâmetro θ, é definida ser um estimador pontual de τ(θ).

Um estimador é sempre uma estatística que é uma função de uma amostra aleatória e que portanto, também é uma variável aleatória. Porém, uma estatística nem sempre é um estimador para
um parâmetro de interesse. Usaremos como notação $\hat{θ}$ para representar um estimador de θ, ou mais
geral, $(\hat{θ}_1,\hat{θ}_2, . . . ,\hat{θ}_k)$ é um vetor de estimadores de $(θ_1, θ_2, . . . , θ_k)$. Diremos também que o valor do estimador é chamado de estimativa pontual.



**Exemplo 1:**

Considere o tempo de vida útil de uma certa bateria automotiva de uma determinada marca, do
qual retirou-se uma amostra de três baterias, cujos valores foram: $X_1 = 1$ ano, $X_2 = 1$, 6 anos e
$X_3 = 1$, 4 anos. Então, em busca de saber a média (µ) do tempo de vida útil da bateria dessa marca,
é intuitivo pensar na média amostral $\bar{X} = 1, 33$ anos como um valor plausível para representar o
parâmetro µ desconhecido, com base nas informações disponíveis na amostra. Assim, o estimador
para µ é $\bar{X} = 1, 33$ anos é sua estimativa.

**Exemplo 2:**

Suponhamos que uma fábrica de lâmpadas produza um certo tipo de lâmpada, e queremos estimar a proporção $p$ de lâmpadas defeituosas.Vamos realizar um teste onde retiramos uma amostra de $e$ lâmpadas da produção e contamos o número de lâmpadas defeituosas.

Denotemos:

* $X$ como o número de lâmpadas defeituosas na amostra de tamanho $n$.

* $X$ segue uma distribuição binomial: $X\sim (n,p)$, onde $p$ é a proporção real de lâmpadas defeituosas que queremos estimar.

Um estimador comum para a proporção $p$ é uma **proporção amostral de defeitos**. Chamaremos este estimador de $\hat{p}$, onde

$$\hat{p}\;=\;\frac{X}{n}$$

**Propriedades do estimador:**

* **Esperança:** A esperança de $\hat{p}$ é igual a $np$, ou seja, $\hat{p}$ é um estimador não viciado de $p$.



* **Variância:** A variância de $\hat{p}$ é dado por:



A variação diminui conforme $n$ aumenta, o que significa que o estimador se torna mais preciso com uma amostra maior.

**Interpretação:**

O estimador $\hat{p}\;=\;\frac{X}{n}$ no dá uma estimativa para a proporção $p$ de lâmpadas defeituosas na população. Com uma amostra grande, podemos confiar que $\hat{p}$ estará próximo de $p$, especialmente considerando as propriedades de não viesamento e variância decrescente do estimador.


## Métodos para obtenção de Estimadores Pontuais.

A estimativa pontual é uma técnica estatística usada para obter um valor específico como uma aproximação de um parâmetro desconhecido da população, como média, variância, proporção, entre outros. Há vários métodos clássicos para encontrar estimadores pontuais de parâmetros, e os mais comuns incluem:

### Método dos Momentos.

Esse método utiliza os momentos da amostra para construir um estimador para o parâmetro da população. Ele se baseia na igualdade entre os momentos da amostra e os momentos teóricos da distribuição.

**Definição: Momentos Amostrais**

Seja uma amostra aleatória $X_1, X_2, . . . , X_n$ com $fdp$ ou $fp f_X(x; θ)$, com $θ = [θ_1, θ_2, . . . , θ_k]'∈ Θ$ em que $Θ$ é o espaço paramétrico. O k-ésimo momento amostral, denotado por $M_k$, é definido por

$$M_k= \frac{1}{n}\;\sum_{i=1}^n\;X_i^k$$

e o k_ésimo momento amostral em torno da média amostral $\bar{X}$, denotado por $M_{k}'$ é definino por

$$M_k'\;= \frac{1}{n}\;\sum_{i=1}^n\;(X-\bar X)^k.$$

**Exemplo 01: Distribuição Exponencial**

A distribuição exponencial com parâmetro $\lambda$ tem a função de densidade:

$$f(x;\lambda)\;=\;\lambda\,e^{-\lambda\,x},com\;x\geq\;0$$

Para a distribuição exponencial, o primeiro momento populacional (ou médio) é:

$$E[X]\;=\;\frac{1}{\lambda}$$

**Igualando ao momento amostral**

$$ \bar{X}\;=\;\frac{1}{n}\sum _{i=1}^n\;X_{i} $$
Seja $\bar{X}$ o momento amostral de ordem 1. Então, pelo método dos momentos, temos

$$\bar{X}\;=\;\frac{1}{\lambda}\;\;e\;\;\hat\lambda\;=\;\frac{1}{\bar X}.$$
**Exemplo no RStudio:**

![](C:/relatorios/profmatma41/imagens/TeXAux/imagem_36.jpg)


**Exemplo 02: Distribuição Normal**

A distribuição normal com média $\mu$ e variação $\sigma^2$ tem função de densidade:


$$f(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2\;}}e^\frac{-(x-\mu)^2}{2\sigma^2}$$
**Igualando os momentos amostrais**

O primeiro momento amostral(média amostral) é: $\bar{X}\;=\;\frac{1}{n}\sum _{i=1}^n\;X_{i}$

O segundo momento amostral é $\bar{X}\;=\;\frac{1}{n}\sum _{i=1}^n\;X^2_{i}$

Pelo método dos momentos, igualamos os momentos amostrais aos populacionais:

Para o primeiro momento: 


$$\bar{X}\;=\;\mu\;\Rightarrow\;\hat{\mu}\;=\;\bar X$$

Para o segundo momento:

$$\frac{1}{n}\sum _{i=1}^n\;X^2_{i}\;=\;\mu^2\;+\;\sigma^2$$

Substituindo $\mu\;=\;\bar X$ , temos:

$$\sigma^2\;=\;\frac{1}{n}\sum_{i=1}^n\;X_i^2\;-\;\bar X^2$$

Assim, o estimador para $\sigma^2$, é


$$\hat{\sigma}\;=\;\frac{1}{n}\sum_{i=1}^n\;X_i^2\;-\;\bar X^2$$


**Encontrando os momentos populacionais**

O primeiro momento populacional é $E[X]\;=\;\mu$

O segundo momento populacional é $E[X^2]\;=\;\mu^2\;+\;\sigma^2.$

**Exemplo no RStudo**

![](C:/relatorios/profmatma41/imagens/TeXAux/imagem_37.jpg)


### Método da Máxima Verossimilhança 

O método da máxima verossimilhança é uma técnica usada para estimar parâmetros de uma distribuição ao maximizar a função de verossimilhança, que representa a probabilidade de observar a amostra dada, considerando o valor do parâmetro a ser estimado.

**Função de Verossimilhança:**

Seja uma amostra aleatória $X_1, X_2, ..., X_n$ com fdp ou fp conjunta $f_X\;(x;\theta)$, com $\theta\; \in\;\Theta$ em que $\Theta$ é o espaço paramétrico. Considere ainda $x_1, x_2, ..., x_n$ a realização da amostra aleatória $X_1, X_2, ..., X_n$, então a função de verossimilhança é definida por:

$$L(\theta;x_1,x_2,...,x_n)\;=\;\prod_{i=1}^n f_X(x_i;\theta).$$

**Log-Verossimilhança:**  Para simplificar os cálculos, aplica-se o logaritmo na função de verossimilhança (log-verossimilhança):

$$ln\; L(\theta)\;=\;\sum_{i=1}^n\;ln\;f(X_i;\theta).$$

**Maximizar a Log-Verossimilhança:** Deriva-se $ln\;L(\theta)$ em relação a $\theta$ e resolve-se $\frac{d}{d\theta}\;ln\;L(\theta)\;=\;0$ para encontrar o valor de $\theta$ que maximiza a função de verossimilhança.


**Exemplo 01:** Distribuição Normal


Suponha que temos uma amostra $ X=\;\{X_1,X_2,...,X_n \}$ de uma distribuição normal $X_i\;\sim (\mu , \sigma^2)$ com variância $\sigma^2$ conhecida.

**Função densidade:** A densidade para cada observação é


$$f(x;\mu)=\frac{1}{\sqrt{2\pi\sigma^2\;}}e^\frac{-(x-\mu)^2}{2\sigma^2}$$
**Função de verossimilhança:** 

$$ln\; L(\mu)\;=\;\sum_{i=1}^n\;ln\;f(X_i;\theta).$$

**Log - verossimilhança:**

$$ln\; L(\mu)\;=-\;\frac{n}{2}\;ln(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n\;(X_i-\mu)^2.$$

**Maximizando a Log-Verossimilhança:**  Derivamos em relação a $\mu$μ e igualamos a zero:

$$\frac{d}{d\mu}\;ln\;L(\mu)\;=\;\frac{1}{\sigma^2}\sum_{i=1}^n\;(X_i-\mu)\;=\;0.$$

Solucionando, obtemos:

$$\hat{\mu}\;=\;\frac{1}{n}\sum_{i=1}^nX_i\;,$$

que é a média amostral $\bar {X}$, nosso estimador de $\mu$ por máxima verossimilhança.


**Exemplo no RStudio**

![](C:/relatorios/profmatma41/imagens/TeXAux/imagem_39.jpg)

**Exemplo 02:** Distribuição Exponencial

Vamos estimar o parâmetro da distribuição exponencial, que possui função densidade de probabilidade dada por:

$$f(x,\lambda)\;=\; \lambda\,e^{-\lambda x}\;, x\geq\;0$$

onde $\lambda\;\geq\;0$ é o parâmetro da taxa, o qual queremos estimar.


**Definindo a função de Verossimilhança:**

Suponha que temos uma amostra $X\;=\;\{X_1, X_2, ..., X_n\}$ de uma variável aleatória exponencial com parâmetro $\lambda$.A função de verossimilhança para $\lambda$, dado $X$,é o produto das densidades para cada observação:



$$L(\lambda)\;=\;\prod_{i=1}^n\;\lambda\,e^{-\lambda X_i}\;=\;\lambda^n\,e^{-\lambda\sum_{i=1}^n\,X_i}.$$
**Log - Verossimilhança:**

Para simplificar, tomamos o logaritmo da função de verossimilhança:

$$ln\,L(\lambda)\;=\;n\,ln(\lambda)-\lambda \sum_{i=1}^n\,X_i.$$

**Maximizar a Log - Verossimilhança:**


Derivamos a log-verossimilhança em relação a $\lambda$e igualamos a zero para encontrar o valor de $\lambda$ que maximiza a verossimilhança:


$$\frac{d}{d\lambda}\,ln\,L(\lambda)\;=\;\frac{n}{\lambda}-\sum_{i=1}^n\,X_i\;=\;0 $$
Resolvendo para $\lambda$, temos

$$\hat \lambda\;=\;\frac{n}{\sum_{i=1}^n\,X_i}\;=\;\frac{1}{\bar X},$$

onde $\bar X\;=\;\frac{1}{n}\sum_{i=1}^n\,X_i$ é a média amostral.

**Exemplo no RStudio**


![](C:/relatorios/profmatma41/imagens/TeXAux/imagem_38.jpg)


### Método dos Mínimos Quadrados

Consiste em um estimador que minimiza a soma dos quadrados dos resíduos da regressão, de forma a maximizar o grau de ajuste do modelo aos dados observados. Um requisito para o método dos mínimos quadrados é que o fator imprevisível (erro) seja distribuído aleatoriamente e essa distribuição seja normal.


**Definição**

Dado um conjunto de dados $(x_i,y_i)$, com $i = 1, 2, ...,n$, queremos encontrar uma função linear da forma:

$$y = \beta_0\;+\;\beta_1x$$

onde:

* $\beta_0$ é o intercepto da reta,

* $\beta_1$ é o coeficiente angular (ou inclinação) da reta.

O objetivo é minimizar a soma dos quadrados dos resíduos, dada por:

$$S\;=\;\sum_{i=1}^n\,(y_i-\hat y_i)^2\;=\;\sum_{i=1}^n\,(y_i\,-\,(\beta_0\;+\;\beta_1\,x_i))^2$$
**Derivando os Estimadores para $\beta_0$ e $\beta_1$**


Para obter os valores que minimizam essa soma de quadrados, calculamos as derivadas parciais de $S$ em relação a $\beta_0$ e $\beta_1$ e as igualamos a zero, resultando no sistema de equações normais.




$$
\left\{
\begin{array}{rcl}
\dfrac{\partial S}{\partial \beta_0}&=&0\\
\dfrac{\partial S}{\partial \beta_1}&=&0\\
\end{array}
\right.
$$

Resolvendo essas equações, encontramos:


* Estimador para $\beta_1:$

$$\hat\beta_1\;=\;\frac{\sum_{i=1}^n\,(x_i-\bar x)\,(y_i-\bar y)}{\sum_{i=1}^n\,(x_i-\bar x)^2}$$

* Estimador para $\beta_0:$


$$\hat \beta_0\;=\;\bar y\,-\,\hat \beta_1x$$


Essas são as estimativas pontuais para os parâmetros $\beta_0$ e $\beta_1$do modelo de regressão linear usando o método dos mínimos quadrados.


**Exemplo no RStudio**

![](C:/relatorios/profmatma41/imagens/TeXAux/imagem_40.jpg)


**Vantagens e Desvantagens dos métodos**


![](C:/relatorios/profmatma41/imagens/TeXAux/imagem_41.jpg)


