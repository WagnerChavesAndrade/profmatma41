---
title: "Relatório 05"
author: "Wagner Andrade"
date: "18/10/2024"
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}
    \includegraphics[width=2in,height=2in]{ufsj.png}\LARGE\\}
  - \posttitle{\end{center}}
toc-title: "Sumário"
output:
  
  html_document:
    theme: journal
    highlight: tango
    toc: yes
    number_sections: yes
    includes:
      in_header: logo.html
  pdf_document:
    
    toc: yes
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
--- 

# Teorema do Limite Central
## Introdução.

O teorema central do limite é um teorema fundamental de probabilidade e estatísticas. O teorema descreve a distribuição da média de uma amostra aleatória de uma população com variância finita. Quando o tamanho amostral é suficientemente grande, a distribuição da média é uma distribuição aproximadamente normal. O teorema aplica-se independentemente da forma da distribuição da população. Muitos procedimentos estatísticos comuns requerem que os dados sejam aproximadamente normais. O teorema central do limite permite a aplicação destes procedimentos úteis a populações que são fortemente não-normais. Quão grande o tamanho amostral deve ser depende da forma da distribuição original. Se a distribuição da população for simétrica, um tamanho amostral de 5 poderia render uma boa aproximação. Se a distribuição da população for fortemente assimétrica, será necessária uma amostra maior. Por exemplo, a distribuição da média pode ser aproximadamente normal, se o tamanho amostral for maior do que 50. Os gráficos a seguir mostram exemplos de como a distribuição afeta o tamanho amostral de que você precisa.


**Amostras de uma população uniforme**

Uma população que segue uma distribuição uniforme é simétrica, mas fortemente não-normal, como mostra o primeiro histograma. Contudo, a distribuição de médias amostrais de 1000 amostras de tamanho 5 desta população é aproximadamente normal devido ao teorema central do limite, como mostra o segundo histograma. Este histograma de médias amostrais inclui uma curva normal sobreposta para ilustrar sua normalidade.

![](C:/relatorios/profmatma41/imagens/TeXAux/imagem_33.jpg)

**Amostras de uma população exponencial**

Uma população que segue uma distribuição exponencial é assimétrica e não-normal, como mostra o primeiro histograma. Contudo, a distribuição de médias amostrais de 1000 amostras de tamanho 50 desta população é aproximadamente normal devido ao teorema central do limite, como mostra o segundo histograma. Este histograma de médias amostrais inclui uma curva normal sobreposta para ilustrar sua normalidade.

![](C:/relatorios/profmatma41/imagens/TeXAux/imagem_34.jpg)

## Enunciado do Teorema do Limite Central

Seja $(X_1\;,X_2\, ,...,\,X_n)$ uma sequência de variáveis aleatórias independentes e identicamente distribuídas com média $\mu$ e variância finita $\sigma^2$. Seja $\;X_1 + X_2 + ... + X_n$ uma amostra da população $X$ com média $\mu$ e variância $\sigma^2$ finita. Então: 

$$Z_n\;=\;\frac{\bar X-\mu}{\frac{\sigma}{\sqrt n}}\longrightarrow\;\mathcal{N}(0,1)\; ,n\longrightarrow \infty.$$
Onde:  

$\bar X\;=\;\sum_{i=1}^{n}\;\frac{X_i}{n}.$ (média amostral)

$\mu$ é a média populacional;

$\sigma$ é o desvio padrão da população;

$n$ é o tamanho da amostra.

### Conceitos preliminares.


$$Z_n\;=\;\frac{\bar X-\mu}{\frac{\sigma}{\sqrt n}}\longrightarrow\;\mathcal{N}(0,1)\; ,n\longrightarrow \infty.$$

$\bar X\;=\;\frac{1}{n}\sum_{i=1}^{n}\;X_i$, onde $X_i$ tem a mesma distribuição que a população, ou seja, é independente e identicamente distribuída, todos tem a mesma distribuição. E essa distribuição tem a mesma média e variância da população:

$$X_i\;\sim (\mu , \sigma^2)$$

Podemos verificar que  $E(\bar X)\;=\;E\left[\frac{1}{n}\sum_{i =1}^n\;X_i  \right]$. Como a Esperança é uma função linear, temos, $E(\bar X)\;=\;\frac{1}{n}E\left[\sum_{i =1}^n\;X_i  \right]$. Temos, também, que a Esperança da soma é igual a soma da Esperança. Logo,

$E(\bar X)\;=\;\frac{1}{n}\sum_{i =1}^n\:E(X_i)\;=\;\;\frac{1}{n}\sum_{i =1}^n\;\mu_i\;=\;\frac{n\mu}{n}\;=\;\mu$.

$Var(\bar X)\;=\;Var\left[\frac{1}{n}\sum_{i =1}^n\;X_i  \right]\;=\;\frac{1}{n^{2}}\sum_{i=1}^n\;Var(X_i)\;=\;\frac{n\sigma^2}{n^2\;}\;=\;\frac{\sigma^2}{n}$

Cálculo da Esperança de $Z_n$

$E(Z_n)\;=\;E\left[\frac{\bar X-\mu}{\frac{\sigma}{\sqrt{n}}} \right]\;=\;\frac{E(\bar X)-\mu}{\frac{\sigma}{\sqrt{n}}}\;=\;\frac{\mu-\mu}{\frac{\sigma}{\sqrt{n}}}\;=\;0$

Cálculo da Variância de $Z_n$

$Var(Z_n)\;=\;Var\left[\frac{\bar X-\mu}{\frac{\sigma}{\sqrt{n}}}\right]\;=\;\frac{Var(\bar X)}{\frac{\sigma^2}{n}}\;=\;\frac{\frac{\sigma^2}{n}}{\frac{\sigma^2}{n}}\;=\;1$.

Concluímos, portanto, que $\;Z_n$ é distribuída com média zero e variância 1. 

$$Z_n\;=\;\longrightarrow\;\mathcal{N}(0,1)(0,1)\; $$
Vamos, agora, encontrar o formato de $Z_n$. Vamos definir uma nova variável, $Y_i$, da seguinte forma:

$Y_i\;=\;\frac{X_i-\mu}{\sigma}$. Com isso, podemos escrever $Z_n$ da seguinte forma:

$Z_n\;=\;\frac{\bar X-\mu}{\frac{\sigma}{\sqrt{n}}}\;\cdot \frac{n}{n}\;=\;\frac{n\bar {X}-n\,\mu}{\sigma\;\sqrt{n}}\;=\;\frac{n\;\sum_{i=1}^{n}\;\frac{X_i}{n}\;-\;n\,\mu}{\sigma\;\sqrt{n}}\;=\;\frac{\sum_{i=1}^{n}\;(X_i\;-\;\mu)}{\sigma\,\sqrt{n}}\;=\;\sum_{i=1}^{n}\;\frac{Y_i}{\sqrt{n}}$.

Isto quer dizer que a nova variável $Y_i$ tem a mesma variância que $Z_n$.

### Função Característica.

Para uma variável aleatória $X$, sua função característica $\phi_{X}(t)$ é definida como

$$\phi_{X}(t)\;=\;E[e^{itX}]$$

* $E[.]$ denota o valor esperado;

* $e^{etX}$ é a função exponencial complexa, com _i_ sendo a unidade imginária ($i = \sqrt{-1}$);

* $t$ é um número real.

A função característica é calculada como o valor esperado da função $e^{itX}$, ou seja, a integral da função exponencial complexa multiplicada pela densidade de probabilidade da variável aleatória $X$,
se ela for contínua:

$$\phi_{X}(t)\;=\;\int_{-\infty}^{+\infty}\;e^{itX}\;f_{X}(x)dx\;=\;M_{X}(t)\;$$( Funão geratriz de momentos)

Aqui, $f_{X}\;(x)$ é a função densidade de probabilidade.

**Propriedades principais:**

**Unicidade:** A função característica determina unicamente a distribuição de uma variável aleatória.

**Existência:** Ela sempre existe, independentemente da existência de momentos.

**Valor zero:** $\phi_X(0)=1$.

**Relação com momentos:** Se os momentos de $X$ existem, eles podem ser derivados a partir da função característica.

**Momento de uma variável de ordem $n$.**

O momento de ordem $n$ de uma variável aleatória $X$, denotado como $E[X^n]$, é o valor esperado da _n_-ésima potência de $X$. Ele fornece informações sobre a distribuição da variável aleatória, como sua dispersão e forma. Os momentos são usados para descrever as características principais de uma distribuição, como sua média, variância, e outros aspectos.

$M_(0)\;=\;\int_{-\infty}^{+\infty}\;e^{0\cdot x}\;f_{X}(x)dx\;=\;\int_{-\infty}^{+\infty}\;f_{X}(x)\;=\;1.$


Derivando em relação a $t$, temos

$d\frac{M_X (t)}{d_t}\left|_{t=0}\right.\;=\;\int_{-\infty}^{+\infty}\;xe^{tx}f(x)dx\left|_{t=0}\right.=\;E(X)\;\longrightarrow\;\frac{d^n}{d\,t^n}M_{X}(t)\left|_{t=0}\right.\;=\;E[X^n]$


Note que, a partir da função característica nós podemos deduzir a função geratriz de momentos e, dessa função geratriz nós podemos calcular todos os momentos da distribuição de probabilidade e, assim, é possível descrever essa distribuição.

Além disso,

$\phi_{a_1X_1+a_2X_2+...+a_nX_n}(t)\;=\;\int\int...\int\;e^{it(a_1+X_1+. . .+a_nX_n)}f(x_1,x_2, . . x_n)dx_1dx_2...dx_n$

Se $X_1, X_2, X_3, ...X_n$ forem independentes, temos que

$\phi_{a_1X_1+a_2X_2+...+a_nX_n}(t)\;=\;\int\int...\int\;e^{ita_1X_1}f_{X_1}(x_1)\;e^{ita_2X_2}f_{X_2}(x_2)dx_1 . . .dx_n$

$\phi_{a_1X_1+a_2X_2+...+a_nX_n}(t)\;=\;\phi_{X_1}(a_1t)\;\phi_{X_2}(a_1t) . . .\phi_{X_n}(a_nt)\;=\;\prod_{i=1}^{n}\;\phi_{X_i}(a_it)$

### Série de Taylor.

Logo,

$\phi_{Z_n}(t)\;=\;\phi_{\sum_{i=1}^n}\;(t)\;=\;\phi_{Y_1}(\frac{t}{\sqrt n})\phi_{Y_2}(\frac{t}{\sqrt n})\;...\;\phi_{Y_n}(\frac{t}{\sqrt n})\;=\;\prod_{i=1}^{n}\phi_{Y_n}(\frac{t}{\sqrt n})\;=\;\left[\phi_{Y_t }\frac{t}{\sqrt n}\right]^n\;\;(1)$

Mas,

$\phi_Y \left(\frac{t}{\sqrt n}\right)\;=\;\int_{-\infty}^{+\infty}e^{i\frac{t}{\sqrt n}Y}f_{Y}(y)d_y$

Expandindo o termo $e^{i\frac{t}{\sqrt n}y}$ em uma série de Taylor, obtemos,

Série de Taylor: $e^x\;=\;\sum_{n=0}^{\infty}\frac{(x)^n}{n!}$

$e^{i\frac{t y}{\sqrt n}}\;=\;1\;+\;\frac{ity}{\sqrt n}\;+\;\left(\frac{ity}{\sqrt n}\right)^2\cdot \frac{1}{2!}\;+\;o(t)^2\;=\;1\;+\;\frac{ity}{\sqrt n}\;+\;\frac{ty^2}{2n}\;+\;o(t)^2$

$\phi_y(\frac{t}{\sqrt n})\;=\;\int_{-\infty}^{\infty}f_Y(y)d_y\;+\;\frac{it}{\sqrt n}\int_{-\infty}^{\infty}yf_{Y}(y)d_y\;-\;\frac{t^2}{2n}\int_{-\infty}^{\infty}y^2f_{Y}(y)d_y\;+\;o(t^2)$

A variável $Y_i$ foi definida como:

$Y_i\;=\;\frac{X_i-\mu}{\sigma}\;\sim\;(0,1)$

$\phi_y(\frac{t}{\sqrt n})\;= 1\;+\;E(Y)\;+\;E(Y^2)\;+\;. . .$

$\phi_y(\frac{t}{\sqrt n})\;= 1\;-\;\frac{t^2}{2n}\;+\;o(t^2)$

Voltando a equação (1), temos:

$\phi{_Z}_{n}(t)\;=\;\left[1\;-\;\frac{t^2}{2n}\;+\;0(t^2)\right]^n$

Quando o tamanho da amostra tende ao infinito, temos:

$\lim_{n\to\infty}\;\phi_{Z_n}(t)\;=\;\lim_{n\to\infty}\;\left[ 1\;-\;\frac{t^2}{2n}\;+\;o(t^2)\right]^n$

Lembrando que, $\lim_{n\to\infty}\;=\;\left(1\;+\;\frac{x}{n}\right)^n\;=\;e^x$, segue que


$\lim_{n\to\infty}\;\phi_{Z_n}(t)\;=\;\lim_{n\to\infty}\;\left[ 1\;-\;\frac{t^2}{2n}\;+\;o(t^2)\right]^n\;=\;e^{-\frac{t^2}{2}}$

Ou seja,

$\lim_{n\to\infty}\;\phi_{Z_n}(t)\;=\;e^{-\frac{t^2}{2}}$




